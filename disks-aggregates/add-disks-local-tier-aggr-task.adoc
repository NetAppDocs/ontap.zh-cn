---
permalink: disks-aggregates/add-disks-local-tier-aggr-task.html 
sidebar: sidebar 
keywords: add, expand, aggregate, add drives, add disks, add capacity, local tier, add disks to local tier, add disks to aggregate, increase storage, increase aggregate size, local tier, capacity, disk, expand storage, add drives, add disks, add capacity 
summary: 您可以将磁盘添加到本地层、以便它可以为与其关联的卷提供更多存储。 
---
= 向ONTAP中的本地层添加容量
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
您可以将磁盘添加到本地层、以便它可以为与其关联的卷提供更多存储。


NOTE: 在ONTAP 9.7之前的版本中、System Manager会使用术语_AGREL_来描述_local ti层_。无论您的ONTAP版本如何、ONTAP命令行界面都使用术语_AGREL_。要了解有关本地层的更多信息，请参见link:../disks-aggregates/index.html["磁盘和本地层"]。

[role="tabbed-block"]
====
.System Manager (ONTAP 9.8及更高版本)
--

NOTE: 从ONTAP 9.12.1开始、您可以使用System Manager查看本地层的已提交容量、以确定本地层是否需要额外容量。请参阅。 link:../concepts/capacity-measurements-in-sm-concept.html["在 System Manager 中监控容量"]

.步骤
. 选择*存储>层*。
. 在要添加容量的本地层的名称旁边选择image:icon_kabob.gif["菜单选项图标"]。
. 选择*添加容量*。
+

NOTE: 如果没有可添加的备用磁盘，则不会显示 * 添加容量 * 选项，并且您无法增加本地层的容量。

. 根据安装的ONTAP 版本执行以下步骤：
+
[cols="30,70"]
|===


| 如果安装的是此版本的ONTAP | 执行以下步骤 ... 


 a| 
ONTAP 9.8、9.9或9.10.1
 a| 
.. 如果节点包含多个存储层，请选择要添加到本地层的磁盘数。  否则，如果节点仅包含一个存储层，则会自动估计添加的容量。
.. 选择 * 添加 * 。




 a| 
从ONTAP 9.11.1开始
 a| 
.. 选择磁盘类型和磁盘数量。
.. 如果要向新RAID组添加磁盘、请选中此复选框。  此时将显示RAID分配。
.. 选择 * 保存 * 。


|===
. （可选）此过程需要一段时间才能完成。如果要在后台运行此进程，请选择 * 在后台运行 * 。
. 完成此过程后，您可以在 * 存储 > 层 * 的本地层信息中查看增加的容量。


--
.System Manager (ONTAP 9.7及更早版本)
--
.步骤
. (仅适用于ONTAP 9.7)选择*(返回经典版本)*。
. 选择*硬件和诊断>聚合*。
. 选择要添加容量磁盘的本地层、然后选择*操作>添加容量*。
+

NOTE: 您应添加与本地层中其他磁盘大小相同的磁盘。

. (仅适用于ONTAP 9.7)选择*切换到新体验*。
. 选择*存储>层*以验证新本地层的大小。


--
.命令行界面
--
.开始之前
您必须了解要添加存储的本地层的RAID组大小是多少。

.关于此任务
此过程用于将已分区磁盘添加到本地层、与添加未分区磁盘的过程类似。

扩展本地层时、您应了解是向本地层添加分区磁盘还是未分区磁盘。将未分区驱动器添加到现有本地层时、新RAID组会继承现有RAID组的大小、这可能会影响所需的奇偶校验磁盘数量。如果将未分区磁盘添加到由分区磁盘组成的 RAID 组中，则新磁盘将进行分区，从而留下一个未使用的备用分区。

配置分区时，您必须确保节点中没有同时包含两个分区的驱动器作为备用驱动器。否则，如果节点发生控制器中断，则可能无法向技术支持提供有关此问题的宝贵信息（核心文件）。

.步骤
. 显示拥有本地层的系统上的可用备用存储：
+
`storage aggregate show-spare-disks -original-owner _node_name_`

+
您可以使用 `-is-disk-shared` 参数、用于仅显示已分区的驱动器或未分区的驱动器。

+
[listing]
----
cl1-s2::> storage aggregate show-spare-disks -original-owner cl1-s2 -is-disk-shared true

Original Owner: cl1-s2
 Pool0
  Shared HDD Spares
                                                            Local    Local
                                                             Data     Root Physical
 Disk                        Type     RPM Checksum         Usable   Usable     Size Status
 --------------------------- ----- ------ -------------- -------- -------- -------- --------
 1.0.1                       BSAS    7200 block           753.8GB  73.89GB  828.0GB zeroed
 1.0.2                       BSAS    7200 block           753.8GB       0B  828.0GB zeroed
 1.0.3                       BSAS    7200 block           753.8GB       0B  828.0GB zeroed
 1.0.4                       BSAS    7200 block           753.8GB       0B  828.0GB zeroed
 1.0.8                       BSAS    7200 block           753.8GB       0B  828.0GB zeroed
 1.0.9                       BSAS    7200 block           753.8GB       0B  828.0GB zeroed
 1.0.10                      BSAS    7200 block                0B  73.89GB  828.0GB zeroed
2 entries were displayed.
----
. 显示本地层的当前RAID组：
+
[source, cli]
----
storage aggregate show-status <aggr_name>
----
+
[listing]
----
cl1-s2::> storage aggregate show-status -aggregate data_1

Owner Node: cl1-s2
 Aggregate: data_1 (online, raid_dp) (block checksums)
  Plex: /data_1/plex0 (online, normal, active, pool0)
   RAID Group /data_1/plex0/rg0 (normal, block checksums)
                                              Usable Physical
     Position Disk        Pool Type     RPM     Size     Size Status
     -------- ----------- ---- ----- ------ -------- -------- ----------
     shared   1.0.10        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.5         0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.6         0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.11        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.0         0   BSAS    7200  753.8GB  828.0GB (normal)
5 entries were displayed.
----
. 模拟向聚合添加存储：
+
[source, cli]
----
storage aggregate add-disks -aggregate <aggr_name> -diskcount <number_of_disks_or_partitions> -simulate true
----
+
您可以在不实际配置任何存储的情况下查看添加存储的结果。如果模拟命令显示任何警告，您可以调整命令并重复模拟。

+
[listing]
----
cl1-s2::> storage aggregate add-disks -aggregate aggr_test -diskcount 5 -simulate true

Disks would be added to aggregate "aggr_test" on node "cl1-s2" in the
following manner:

First Plex

  RAID Group rg0, 5 disks (block checksum, raid_dp)
                                                      Usable Physical
    Position   Disk                      Type           Size     Size
    ---------- ------------------------- ---------- -------- --------
    shared     1.11.4                    SSD         415.8GB  415.8GB
    shared     1.11.18                   SSD         415.8GB  415.8GB
    shared     1.11.19                   SSD         415.8GB  415.8GB
    shared     1.11.20                   SSD         415.8GB  415.8GB
    shared     1.11.21                   SSD         415.8GB  415.8GB

Aggregate capacity available for volume use would be increased by 1.83TB.
----
. 将存储添加到聚合：
+
[source, cli]
----
storage aggregate add-disks -aggregate <aggr_name> -raidgroup new -diskcount <number_of_disks_or_partitions>
----
+
创建Flash Pool本地层时、如果要添加的磁盘校验和与本地层不同、或者要向混合校验和本地层添加磁盘、则必须使用参数。 `-checksumstyle`

+
如果要向Flash Pool本地层添加磁盘、则必须使用 `-disktype`参数指定磁盘类型。

+
您可以使用 `-disksize`参数指定要添加的磁盘的大小。只会选择具有近似指定大小的磁盘添加到本地层。

+
[listing]
----
cl1-s2::> storage aggregate add-disks -aggregate data_1 -raidgroup new -diskcount 5
----
. 验证是否已成功添加存储：
+
[source, cli]
----
storage aggregate show-status -aggregate <aggr_name>
----
+
[listing]
----
cl1-s2::> storage aggregate show-status -aggregate data_1

Owner Node: cl1-s2
 Aggregate: data_1 (online, raid_dp) (block checksums)
  Plex: /data_1/plex0 (online, normal, active, pool0)
   RAID Group /data_1/plex0/rg0 (normal, block checksums)
                                                              Usable Physical
     Position Disk                        Pool Type     RPM     Size     Size Status
     -------- --------------------------- ---- ----- ------ -------- -------- ----------
     shared   1.0.10                       0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.5                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.6                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.11                       0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.0                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.2                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.3                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.4                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.8                        0   BSAS    7200  753.8GB  828.0GB (normal)
     shared   1.0.9                        0   BSAS    7200  753.8GB  828.0GB (normal)
10 entries were displayed.
----
. 验证节点是否仍至少有一个驱动器，其中根分区和数据分区均为备用驱动器：
+
[source, cli]
----
storage aggregate show-spare-disks -original-owner <node_name>
----
+
[listing]
----
cl1-s2::> storage aggregate show-spare-disks -original-owner cl1-s2 -is-disk-shared true

Original Owner: cl1-s2
 Pool0
  Shared HDD Spares
                                                            Local    Local
                                                             Data     Root Physical
 Disk                        Type     RPM Checksum         Usable   Usable     Size Status
 --------------------------- ----- ------ -------------- -------- -------- -------- --------
 1.0.1                       BSAS    7200 block           753.8GB  73.89GB  828.0GB zeroed
 1.0.10                      BSAS    7200 block                0B  73.89GB  828.0GB zeroed
2 entries were displayed.
----


--
====